{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape):\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(1,1),activation=\"relu\",input_shape=shape),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2,2), padding='valid'),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(2,2),strides=(1,1),activation=\"relu\"),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1,1), padding='valid'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    \n",
    "    ])\n",
    "    \n",
    "    loss_fn =tf.keras.losses.MeanSquaredError()\n",
    "    model.compile(optimizer=\"adam\",\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_des(y_hat,y,model):\n",
    "    model.fit(np.array([y_hat]), np.reshape(y,(1,)) ,verbose=0)\n",
    "def forward_prop(state,model):\n",
    "    predictions = model(np.array([state])).numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    return q_values.index(max(q_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent():\n",
    "    \n",
    "    def agent_init(self, agent_info):\n",
    "        self.input_shape= agent_info.get(\"input_shape\",(10,10,1))\n",
    "        self.num_actions = agent_info.get(\"num_actions\",4)\n",
    "        self.gamma = agent_info.get(\"discount\", 0.97)\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.001)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.hunger=agent_info.get(\"hunger\", 50)\n",
    "        self.epsilon_decay=agent_info.get(\"epsilon_decay\",10**-5)\n",
    "        self.min_epsilon=agent_info.get(\"min_epsilon\",0.001)\n",
    "        self.modelQueue_capacity=agent_info.get(\"modelQueue_capacity\", 20000)\n",
    "        self.batch_size=agent_info.get(\"batch_size\", 10)\n",
    "        self.model_planing_preiod=agent_info.get(\"model_planing_preiod\", 4)\n",
    "        self.actions = list(range(self.num_actions))\n",
    "        self.modelQueue = []\n",
    "        self.models=[]\n",
    "        self.previous_state=None\n",
    "        self.last_action=None\n",
    "\n",
    "        self.iteration_count=1\n",
    "        for i in range(self.num_actions):\n",
    "            self.models.append(create_model(self.input_shape))\n",
    "    def select_action(self, state):\n",
    "        chosen_action = None\n",
    "        action_values=[]\n",
    "        save_epsilon=self.epsilon\n",
    "        if(self.hunger>50):\n",
    "            self.epsilon=0.2\n",
    "        for x in self.models:\n",
    "            action_values.append(forward_prop(state,x))\n",
    "        if np.random.random() > self.epsilon: \n",
    "            chosen_action = argmax(action_values)\n",
    "        else: \n",
    "            chosen_action = np.random.choice(self.num_actions)\n",
    "        self.epsilon=save_epsilon\n",
    "        self.epsilon-=self.epsilon_decay\n",
    "        self.hunger+=1\n",
    "        self.epsilon=max(self.min_epsilon,self.epsilon)\n",
    "        return chosen_action, action_values[chosen_action],max(action_values)\n",
    "    def model_update(self,state,action,reward,new_state):\n",
    "        elemant=[(state,action),(reward,new_state)] \n",
    "        i=0\n",
    "        for x in self.modelQueue:\n",
    "            \n",
    "            comparison=elemant[0][0]==x[0][0]\n",
    "            if comparison.all() and elemant[0][1]==x[0][1]:\n",
    "                self.modelQueue.pop(i)\n",
    "                self.iteration_count+=1\n",
    "                self.modelQueue.append(elemant)\n",
    "                return\n",
    "            i+=1\n",
    "        if (len(self.modelQueue))==self.modelQueue_capacity:\n",
    "            self.modelQueue.pop(0)\n",
    "        self.modelQueue.append(elemant)\n",
    "        self.iteration_count+=1\n",
    "    def planing_step(self):\n",
    "        batch=np.random.choice(np.array(self.modelQueue).shape[0],self.batch_size)\n",
    "        for x in batch:\n",
    "            past_state=self.modelQueue[x][0][0]\n",
    "            action=self.modelQueue[x][0][1]\n",
    "            reward=self.modelQueue[x][1][0]\n",
    "            state=self.modelQueue[x][1][1]\n",
    "            if(state==-1):\n",
    "                y=reward\n",
    "            else:\n",
    "                y=reward+self.gamma*state\n",
    "            grad_des(past_state,y,self.models[action])\n",
    "    def start(self,state):\n",
    "        self.hunger=0\n",
    "        current_action , self.lastActionValue,maxActionValue= self.select_action(state)\n",
    "        self.last_action = current_action\n",
    "        self.previous_state = state\n",
    "        return self.last_action\n",
    "    def step(self,reward,state):\n",
    "        if reward==1:\n",
    "            self.hunger=0\n",
    "        current_action,action_value,maxActionValue=self.select_action(state)\n",
    "        y=reward+self.gamma*maxActionValue\n",
    "        grad_des(self.previous_state,y,self.models[self.last_action])\n",
    "        self.model_update(self.previous_state,self.last_action,reward,maxActionValue)\n",
    "        if(self.iteration_count%self.model_planing_preiod==0):\n",
    "            self.planing_step()\n",
    "        self.last_action=current_action\n",
    "        self.previous_state=state\n",
    "        return self.last_action\n",
    "    def end(self,reward):\n",
    "        y=reward\n",
    "        grad_des(self.previous_state,y,self.models[self.last_action])\n",
    "        self.model_update(self.previous_state,self.last_action,reward,-1)\n",
    "        if(self.iteration_count%self.model_planing_preiod==0):\n",
    "            self.planing_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
