{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#max 27\n",
    "#sonra devam et\n",
    "#env değiştirildi. \n",
    "#best snakes so far tmp/Actor_v2.0-copy1-savepoint3 tmp/Critic_v2.0-copy1-savepoint3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from envwithseen0gamma.ipynb\n",
      "False\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [5. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "-0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import import_ipynb\n",
    "from envwithseen0gamma import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "env=environement()\n",
    "info={}\n",
    "env.env_init(info)\n",
    "\n",
    "class a2c_v2:\n",
    "    def __init__(self, info={}):\n",
    "        self.n_actions = info.get(\"n_actions\",4) \n",
    "        self.input_shape=info.get(\"input_shape\",(10,10,2)) \n",
    "        self.EPISODES = info.get(\"episodes\",100000) \n",
    "        self.lr = info.get(\"lr\",0.001) \n",
    "        self.gamma=info.get(\"gamma\",0.99)\n",
    "        self.Actor, self.Critic = self.create_model(input_shape=self.input_shape, n_actions=self.n_actions, lr=self.lr)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        \n",
    "    def create_model(self,input_shape,n_actions,lr):\n",
    "        inputs= keras.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 4, activation=\"relu\",padding=\"same\")(inputs)\n",
    "        x=layers.MaxPooling2D(2)(x)\n",
    "        x=layers.Conv2D(128, 2, activation=\"relu\")(x)\n",
    "        x=layers.MaxPooling2D(2)(x)\n",
    "        x=layers.Flatten()(x)\n",
    "        x=layers.Dense(256,activation=\"relu\")(x)\n",
    "        action = layers.Dense(n_actions, activation=\"softmax\")(x)\n",
    "        value = layers.Dense(1)(x)\n",
    "\n",
    "        Actor = keras.Model(inputs = inputs, outputs = action)\n",
    "        Actor.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=lr))\n",
    "\n",
    "        Critic = keras.Model(inputs = inputs, outputs = value)\n",
    "        Critic.compile(loss='mse', optimizer=keras.optimizers.RMSprop(lr=lr))\n",
    "\n",
    "        return Actor, Critic\n",
    "    def remember(self, state, action, reward):\n",
    "        self.states.append(np.reshape(state,(1,10,10,2)))\n",
    "        action_onehot = np.zeros([self.n_actions])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "    def act(self, state):\n",
    "        prediction = self.Actor.predict(np.reshape(state,(1,10,10,2)))[0]\n",
    "        #print(prediction)\n",
    "        action = np.random.choice(self.n_actions, p=prediction)\n",
    "        return action\n",
    "    def discount_rewards(self, reward,states):\n",
    "        gamma=0.01*env.length    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward,dtype=float)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "        return discounted_r\n",
    "        \n",
    "    def replay(self):\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "        discounted_r = self.discount_rewards(self.rewards,states)\n",
    "        values = self.Critic.predict(states)[:, 0]\n",
    "        advantages = discounted_r - values\n",
    "        # training Actor and Critic networks\n",
    "        self.Actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "        self.Critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    def step(self, action):\n",
    "        reward, next_state, done,score = env.step(action)\n",
    "        return next_state, reward, done,score\n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = env.start()\n",
    "            done = False\n",
    "            i=0\n",
    "            while not done:\n",
    "                i+=1\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done,score= self.step(action)\n",
    "                #print(state[:,:,0])\n",
    "                self.remember(state, action, reward)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print(\"Episode :\",e,\" Score :\",score,\"length :\" , i)\n",
    "                    if(e%1000==0):\n",
    "                        self.save()\n",
    "                    self.replay()\n",
    "    def save(self):\n",
    "        self.Actor.save('tmp/Actorway')\n",
    "        self.Critic.save('tmp/Criticway')\n",
    "    def load(self):\n",
    "        self.Actor = tf.keras.models.load_model('tmp/Actorway')\n",
    "        self.Critic = tf.keras.models.load_model('tmp/Criticway')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 10, 2)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 10, 10, 32)        1056      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 128)         16512     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 149,153\n",
      "Trainable params: 149,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a=a2c_v2()\n",
    "a.Critic.summary()\n",
    "a.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0  Score : 8 length : 60\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: tmp/Actorway\\assets\n",
      "INFO:tensorflow:Assets written to: tmp/Criticway\\assets\n",
      "Episode : 1  Score : 8 length : 72\n",
      "Episode : 2  Score : 8 length : 71\n",
      "Episode : 3  Score : 16 length : 143\n",
      "Episode : 4  Score : 1 length : 12\n",
      "Episode : 5  Score : 12 length : 112\n",
      "Episode : 6  Score : 5 length : 51\n",
      "Episode : 7  Score : 11 length : 91\n",
      "Episode : 8  Score : 9 length : 74\n",
      "Episode : 9  Score : 11 length : 81\n",
      "Episode : 10  Score : 13 length : 126\n",
      "Episode : 11  Score : 18 length : 189\n",
      "Episode : 12  Score : 10 length : 77\n",
      "Episode : 13  Score : 12 length : 123\n",
      "Episode : 14  Score : 4 length : 38\n",
      "Episode : 15  Score : 12 length : 83\n",
      "Episode : 16  Score : 9 length : 82\n",
      "Episode : 17  Score : 8 length : 86\n",
      "Episode : 18  Score : 5 length : 34\n",
      "Episode : 19  Score : 13 length : 112\n",
      "Episode : 20  Score : 5 length : 43\n",
      "Episode : 21  Score : 12 length : 92\n",
      "Episode : 22  Score : 7 length : 52\n",
      "Episode : 23  Score : 11 length : 73\n",
      "Episode : 24  Score : 11 length : 79\n",
      "Episode : 25  Score : 2 length : 16\n",
      "Episode : 26  Score : 4 length : 34\n",
      "Episode : 27  Score : 9 length : 75\n",
      "Episode : 28  Score : 7 length : 86\n",
      "Episode : 29  Score : 4 length : 41\n",
      "Episode : 30  Score : 6 length : 39\n",
      "Episode : 31  Score : 5 length : 44\n",
      "Episode : 32  Score : 3 length : 28\n",
      "Episode : 33  Score : 8 length : 53\n",
      "Episode : 34  Score : 2 length : 15\n",
      "Episode : 35  Score : 15 length : 124\n",
      "Episode : 36  Score : 8 length : 75\n",
      "Episode : 37  Score : 15 length : 119\n",
      "Episode : 38  Score : 14 length : 122\n",
      "Episode : 39  Score : 3 length : 45\n",
      "Episode : 40  Score : 9 length : 52\n",
      "Episode : 41  Score : 8 length : 63\n",
      "Episode : 42  Score : 11 length : 87\n",
      "Episode : 43  Score : 10 length : 76\n",
      "Episode : 44  Score : 11 length : 66\n",
      "Episode : 45  Score : 13 length : 113\n",
      "Episode : 46  Score : 8 length : 83\n",
      "Episode : 47  Score : 0 length : 11\n",
      "Episode : 48  Score : 12 length : 136\n",
      "Episode : 49  Score : 14 length : 112\n",
      "Episode : 50  Score : 6 length : 47\n",
      "Episode : 51  Score : 0 length : 6\n",
      "Episode : 52  Score : 8 length : 51\n",
      "Episode : 53  Score : 10 length : 80\n",
      "Episode : 54  Score : 12 length : 69\n",
      "Episode : 55  Score : 11 length : 104\n",
      "Episode : 56  Score : 8 length : 75\n",
      "Episode : 57  Score : 2 length : 17\n",
      "Episode : 58  Score : 9 length : 67\n",
      "Episode : 59  Score : 3 length : 33\n",
      "Episode : 60  Score : 7 length : 53\n",
      "Episode : 61  Score : 13 length : 106\n",
      "Episode : 62  Score : 11 length : 96\n",
      "Episode : 63  Score : 10 length : 93\n",
      "Episode : 64  Score : 10 length : 70\n",
      "Episode : 65  Score : 3 length : 30\n",
      "Episode : 66  Score : 9 length : 68\n",
      "Episode : 67  Score : 9 length : 70\n",
      "Episode : 68  Score : 2 length : 22\n",
      "Episode : 69  Score : 13 length : 123\n",
      "Episode : 70  Score : 15 length : 111\n",
      "Episode : 71  Score : 15 length : 104\n",
      "Episode : 72  Score : 7 length : 39\n",
      "Episode : 73  Score : 15 length : 115\n",
      "Episode : 74  Score : 6 length : 43\n"
     ]
    }
   ],
   "source": [
    "a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
